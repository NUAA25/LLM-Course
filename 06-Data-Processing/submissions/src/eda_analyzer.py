"""
æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)æ¨¡å—
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Any, Tuple
from rich.console import Console
from rich.table import Table
import warnings
warnings.filterwarnings('ignore')

console = Console()

class EDAAnalyzer:
    """EDAåˆ†æå™¨"""
    
    def __init__(self, llm_agent=None):
        self.llm_agent = llm_agent
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")
        
    def analyze(self, data: pd.DataFrame, output_dir: str = "results") -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„EDAåˆ†æ
        
        Args:
            data: è¾“å…¥æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            EDAåˆ†æç»“æœ
        """
        console.print("[bold blue]ğŸ” å¼€å§‹æ¢ç´¢æ€§æ•°æ®åˆ†æ...[/bold blue]")
        
        results = {
            'summary': {},
            'distributions': {},
            'correlations': {},
            'patterns': {},
            'insights': []
        }
        
        # 1. æ•°æ®æ¦‚è§ˆ
        results['summary'] = self._get_data_summary(data)
        
        # 2. å•å˜é‡åˆ†æ
        console.print("ğŸ“Š å•å˜é‡åˆ†æ...")
        numerical_cols = data.select_dtypes(include=[np.number]).columns
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns
        
        for col in numerical_cols:
            results['distributions'][col] = self._analyze_numerical_column(data, col, output_dir)
        
        for col in categorical_cols:
            results['distributions'][col] = self._analyze_categorical_column(data, col, output_dir)
        
        # 3. åŒå˜é‡åˆ†æ
        console.print("ğŸ“ˆ åŒå˜é‡åˆ†æ...")
        if len(numerical_cols) > 1:
            results['correlations'] = self._analyze_correlations(data, numerical_cols, output_dir)
        
        # 4. å¤šå˜é‡åˆ†æ
        console.print("ğŸŒ å¤šå˜é‡åˆ†æ...")
        if len(numerical_cols) >= 3:
            results['patterns'] = self._analyze_multivariate_patterns(data, numerical_cols, output_dir)
        
        # 5. ä½¿ç”¨LLMç”Ÿæˆæ´å¯Ÿ
        if self.llm_agent:
            console.print("ğŸ¤– ä½¿ç”¨LLMç”Ÿæˆæ•°æ®æ´å¯Ÿ...")
            insights = self._generate_insights_with_llm(data, results)
            results['insights'] = insights
        
        # 6. ç”ŸæˆEDAæŠ¥å‘Š
        self._generate_eda_report(results, output_dir)
        
        console.print("[green]âœ… EDAåˆ†æå®Œæˆ[/green]")
        return results
    
    def _get_data_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦"""
        summary = {
            'shape': data.shape,
            'dtypes': data.dtypes.to_dict(),
            'memory_usage': data.memory_usage(deep=True).sum(),
            'missing_values': data.isnull().sum().to_dict(),
            'missing_percentage': (data.isnull().sum() / len(data) * 100).to_dict(),
            'numerical_stats': {},
            'categorical_stats': {}
        }
        
        # æ•°å€¼å‹æ•°æ®ç»Ÿè®¡
        numerical_cols = data.select_dtypes(include=[np.number]).columns
        if len(numerical_cols) > 0:
            summary['numerical_stats'] = data[numerical_cols].describe().to_dict()
        
        # åˆ†ç±»å‹æ•°æ®ç»Ÿè®¡
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            for col in categorical_cols:
                summary['categorical_stats'][col] = {
                    'unique_count': data[col].nunique(),
                    'top_values': data[col].value_counts().head(5).to_dict()
                }
        
        return summary
    
    def _analyze_numerical_column(self, data: pd.DataFrame, column: str, 
                                output_dir: str) -> Dict[str, Any]:
        """åˆ†ææ•°å€¼å‹åˆ—"""
        results = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = data[column].describe().to_dict()
        stats['skewness'] = data[column].skew()
        stats['kurtosis'] = data[column].kurtosis()
        stats['cv'] = data[column].std() / data[column].mean() if data[column].mean() != 0 else np.nan
        
        results['statistics'] = stats
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ç›´æ–¹å›¾
        axes[0, 0].hist(data[column].dropna(), bins=50, edgecolor='black', alpha=0.7)
        axes[0, 0].set_title(f'{column} - Distribution', fontsize=12)
        axes[0, 0].set_xlabel(column)
        axes[0, 0].set_ylabel('Frequency')
        
        # ç®±çº¿å›¾
        axes[0, 1].boxplot(data[column].dropna())
        axes[0, 1].set_title(f'{column} - Box Plot', fontsize=12)
        axes[0, 1].set_ylabel(column)
        
        # Q-Qå›¾
        from scipy import stats
        stats.probplot(data[column].dropna(), dist="norm", plot=axes[1, 0])
        axes[1, 0].set_title(f'{column} - Q-Q Plot', fontsize=12)
        
        # æ ¸å¯†åº¦ä¼°è®¡
        sns.kdeplot(data[column].dropna(), ax=axes[1, 1], fill=True)
        axes[1, 1].set_title(f'{column} - KDE Plot', fontsize=12)
        axes[1, 1].set_xlabel(column)
        
        plt.suptitle(f'Analysis of {column}', fontsize=16)
        plt.tight_layout()
        plt.savefig(f'{output_dir}/figures/{column}_distribution.png', dpi=300)
        plt.close()
        
        # äº¤äº’å¼å›¾è¡¨ (Plotly)
        fig = px.histogram(data, x=column, marginal='box', 
                         title=f'Distribution of {column}')
        fig.write_html(f'{output_dir}/figures/{column}_interactive.html')
        
        return results
    
    def _analyze_categorical_column(self, data: pd.DataFrame, column: str, 
                                  output_dir: str) -> Dict[str, Any]:
        """åˆ†æåˆ†ç±»å‹åˆ—"""
        results = {}
        
        value_counts = data[column].value_counts()
        results['value_counts'] = value_counts.to_dict()
        results['unique_count'] = data[column].nunique()
        results['top_value'] = value_counts.index[0] if len(value_counts) > 0 else None
        results['top_percentage'] = (value_counts.iloc[0] / len(data) * 100) if len(value_counts) > 0 else 0
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ¡å½¢å›¾
        top_n = min(20, len(value_counts))
        axes[0].barh(range(top_n), value_counts.head(top_n).values)
        axes[0].set_yticks(range(top_n))
        axes[0].set_yticklabels(value_counts.head(top_n).index)
        axes[0].set_title(f'{column} - Top {top_n} Categories', fontsize=12)
        axes[0].set_xlabel('Count')
        
        # é¥¼å›¾ (ä»…æ˜¾ç¤ºå‰5ä¸ªç±»åˆ«)
        top_5 = value_counts.head(5)
        other_sum = value_counts[5:].sum() if len(value_counts) > 5 else 0
        
        if other_sum > 0:
            top_5['Other'] = other_sum
        
        axes[1].pie(top_5.values, labels=top_5.index, autopct='%1.1f%%')
        axes[1].set_title(f'{column} - Distribution', fontsize=12)
        
        plt.suptitle(f'Analysis of {column}', fontsize=16)
        plt.tight_layout()
        plt.savefig(f'{output_dir}/figures/{column}_categorical.png', dpi=300)
        plt.close()
        
        return results
    
    def _analyze_correlations(self, data: pd.DataFrame, numerical_cols: List[str], 
                            output_dir: str) -> Dict[str, Any]:
        """åˆ†æç›¸å…³æ€§"""
        results = {}
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = data[numerical_cols].corr()
        results['correlation_matrix'] = corr_matrix.to_dict()
        
        # æ‰¾å‡ºå¼ºç›¸å…³æ€§
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:
                    strong_correlations.append({
                        'pair': (corr_matrix.columns[i], corr_matrix.columns[j]),
                        'correlation': corr_value
                    })
        
        results['strong_correlations'] = strong_correlations
        
        # å¯è§†åŒ–
        plt.figure(figsize=(12, 10))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', 
                   cmap='coolwarm', center=0, square=True, linewidths=0.5)
        plt.title('Correlation Matrix', fontsize=16)
        plt.tight_layout()
        plt.savefig(f'{output_dir}/figures/correlation_matrix.png', dpi=300)
        plt.close()
        
        # æ•£ç‚¹å›¾çŸ©é˜µ
        if len(numerical_cols) <= 8:  # é¿å…å›¾å¤ªå¤§
            scatter_matrix = pd.plotting.scatter_matrix(
                data[numerical_cols], 
                figsize=(15, 15),
                diagonal='kde',
                alpha=0.5
            )
            plt.suptitle('Scatter Matrix of Numerical Variables', fontsize=16)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/figures/scatter_matrix.png', dpi=300)
            plt.close()
        
        return results
    
    def _analyze_multivariate_patterns(self, data: pd.DataFrame, 
                                     numerical_cols: List[str], 
                                     output_dir: str) -> Dict[str, Any]:
        """åˆ†æå¤šå˜é‡æ¨¡å¼"""
        results = {}
        
        # PCAåˆ†æ
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler
        
        # æ ‡å‡†åŒ–æ•°æ®
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data[numerical_cols].fillna(0))
        
        # æ‰§è¡ŒPCA
        pca = PCA()
        pca_result = pca.fit_transform(scaled_data)
        
        results['pca'] = {
            'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
            'cumulative_variance': np.cumsum(pca.explained_variance_ratio_).tolist(),
            'components': pca.components_.tolist()
        }
        
        # å¯è§†åŒ–PCAç»“æœ
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # è§£é‡Šæ–¹å·®æ¯”
        axes[0].plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    pca.explained_variance_ratio_, 'bo-')
        axes[0].set_xlabel('Principal Component')
        axes[0].set_ylabel('Explained Variance Ratio')
        axes[0].set_title('Scree Plot')
        axes[0].grid(True)
        
        # ç´¯ç§¯è§£é‡Šæ–¹å·®
        axes[1].plot(range(1, len(pca.explained_variance_ratio_) + 1), 
                    np.cumsum(pca.explained_variance_ratio_), 'ro-')
        axes[1].set_xlabel('Number of Components')
        axes[1].set_ylabel('Cumulative Explained Variance')
        axes[1].set_title('Cumulative Explained Variance')
        axes[1].grid(True)
        
        plt.suptitle('PCA Analysis', fontsize=16)
        plt.tight_layout()
        plt.savefig(f'{output_dir}/figures/pca_analysis.png', dpi=300)
        plt.close()
        
        return results
    
    def _generate_insights_with_llm(self, data: pd.DataFrame, 
                                  eda_results: Dict[str, Any]) -> List[str]:
        """ä½¿ç”¨LLMç”Ÿæˆæ´å¯Ÿ"""
        if not self.llm_agent:
            return []
        
        # å‡†å¤‡æ•°æ®æ‘˜è¦
        data_summary = f"""
        æ•°æ®é›†ä¿¡æ¯:
        - å½¢çŠ¶: {eda_results['summary']['shape']}
        - æ•°å€¼åˆ—æ•°: {len(data.select_dtypes(include=[np.number]).columns)}
        - åˆ†ç±»åˆ—æ•°: {len(data.select_dtypes(include=['object', 'category']).columns)}
        - ç¼ºå¤±å€¼æ¯”ä¾‹: {data.isnull().sum().sum() / data.size:.2%}
        
        å…³é”®å‘ç°:
        - å¼ºç›¸å…³æ€§: {len(eda_results.get('correlations', {}).get('strong_correlations', []))} å¯¹
        - åæ€åˆ†å¸ƒ: {sum([1 for col, stats in eda_results['distributions'].items() 
                         if 'statistics' in stats and abs(stats['statistics'].get('skewness', 0)) > 1])}
        """
        
        # è·å–LLMæ´å¯Ÿ
        response = self.llm_agent.analyze_data(
            task_description="åŸºäºEDAç»“æœç”Ÿæˆæ•°æ®æ´å¯Ÿå’Œå»ºè®®",
            data_context=data_summary,
            analysis_type="eda_insights"
        )
        
        if response and 'insights' in response:
            insights = response['insights'].split('\n')
            return [insight.strip() for insight in insights if insight.strip()]
        
        return []
    
    def _generate_eda_report(self, results: Dict[str, Any], output_dir: str):
        """ç”ŸæˆEDAæŠ¥å‘Š"""
        report_path = f"{output_dir}/eda_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# EDAåˆ†ææŠ¥å‘Š\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            f.write("## 1. æ•°æ®æ¦‚è§ˆ\n\n")
            f.write(f"- **æ•°æ®å½¢çŠ¶**: {results['summary']['shape']}\n")
            f.write(f"- **å†…å­˜ä½¿ç”¨**: {results['summary']['memory_usage'] / 1024**2:.2f} MB\n")
            
            # ç¼ºå¤±å€¼åˆ†æ
            missing_info = results['summary']['missing_percentage']
            missing_cols = {k: v for k, v in missing_info.items() if v > 0}
            
            if missing_cols:
                f.write("\n## 2. ç¼ºå¤±å€¼åˆ†æ\n\n")
                f.write("| åˆ—å | ç¼ºå¤±ç™¾åˆ†æ¯” |\n")
                f.write("|------|------------|\n")
                for col, pct in missing_cols.items():
                    f.write(f"| {col} | {pct:.2f}% |\n")
            
            # åˆ†å¸ƒåˆ†æ
            f.write("\n## 3. å˜é‡åˆ†å¸ƒ\n\n")
            
            for col, stats in results['distributions'].items():
                if 'statistics' in stats:  # æ•°å€¼å‹å˜é‡
                    stat_info = stats['statistics']
                    f.write(f"### {col}\n")
                    f.write(f"- **ååº¦**: {stat_info.get('skewness', 'N/A'):.4f}\n")
                    f.write(f"- **å³°åº¦**: {stat_info.get('kurtosis', 'N/A'):.4f}\n")
                    f.write(f"- **å˜å¼‚ç³»æ•°**: {stat_info.get('cv', 'N/A'):.4f}\n\n")
                elif 'value_counts' in stats:  # åˆ†ç±»å‹å˜é‡
                    f.write(f"### {col}\n")
                    f.write(f"- **å”¯ä¸€å€¼æ•°é‡**: {stats['unique_count']}\n")
                    f.write(f"- **æœ€å¸¸è§å€¼**: {stats['top_value']} ({stats['top_percentage']:.2f}%)\n\n")
            
            # ç›¸å…³æ€§åˆ†æ
            if 'correlations' in results:
                f.write("\n## 4. ç›¸å…³æ€§åˆ†æ\n\n")
                
                strong_corrs = results['correlations'].get('strong_correlations', [])
                if strong_corrs:
                    f.write("### å¼ºç›¸å…³æ€§å¯¹ (|r| > 0.7)\n\n")
                    f.write("| å˜é‡å¯¹ | ç›¸å…³ç³»æ•° |\n")
                    f.write("|--------|----------|\n")
                    for corr in strong_corrs:
                        pair = corr['pair']
                        f.write(f"| {pair[0]} - {pair[1]} | {corr['correlation']:.4f} |\n")
            
            # LLMæ´å¯Ÿ
            if results['insights']:
                f.write("\n## 5. AIæ´å¯Ÿä¸å»ºè®®\n\n")
                for i, insight in enumerate(results['insights'], 1):
                    f.write(f"{i}. {insight}\n")
            
            # å¯è§†åŒ–æ–‡ä»¶åˆ—è¡¨
            f.write("\n## 6. ç”Ÿæˆçš„å¯è§†åŒ–æ–‡ä»¶\n\n")
            import os
            figure_dir = f"{output_dir}/figures"
            if os.path.exists(figure_dir):
                figures = [f for f in os.listdir(figure_dir) if f.endswith(('.png', '.html'))]
                for figure in figures:
                    f.write(f"- `{figure_dir}/{figure}`\n")
        
        console.print(f"[green]ğŸ“„ EDAæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}[/green]")
    
    def interactive_visualization(self, data: pd.DataFrame):
        """äº¤äº’å¼å¯è§†åŒ–"""
        console.print("[bold yellow]ğŸ¨ äº¤äº’å¼å¯è§†åŒ–[/bold yellow]")
        
        numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        while True:
            console.print("\n[bold cyan]å¯è§†åŒ–é€‰é¡¹:[/bold cyan]")
            console.print("1. å•å˜é‡åˆ†å¸ƒ")
            console.print("2. åŒå˜é‡å…³ç³»")
            console.print("3. ç›¸å…³æ€§çƒ­å›¾")
            console.print("4. æ—¶é—´åºåˆ—åˆ†æ")
            console.print("5. å¤šå˜é‡åˆ†æ")
            console.print("6. è‡ªå®šä¹‰Plotlyå›¾è¡¨")
            console.print("7. è¿”å›")
            
            choice = console.input("[bold cyan]è¯·é€‰æ‹© (1-7): [/bold cyan]").strip()
            
            if choice == "1":
                self._univariate_visualization_interactive(data, numerical_cols, categorical_cols)
            elif choice == "2":
                self._bivariate_visualization_interactive(data, numerical_cols, categorical_cols)
            elif choice == "3":
                self._plot_correlation_matrix_interactive(data, numerical_cols)
            elif choice == "4":
                self._time_series_analysis_interactive(data)
            elif choice == "5":
                self._multivariate_analysis_interactive(data, numerical_cols)
            elif choice == "6":
                self._custom_plotly_visualization_interactive(data)
            elif choice == "7":
                break
            else:
                console.print("[red]æ— æ•ˆé€‰æ‹©[/red]")
    
    def _univariate_visualization_interactive(self, data, numerical_cols, categorical_cols):
        """äº¤äº’å¼å•å˜é‡å¯è§†åŒ–"""
        console.print("\n[bold]é€‰æ‹©å˜é‡ç±»å‹:[/bold]")
        console.print("1. æ•°å€¼å˜é‡")
        console.print("2. åˆ†ç±»å˜é‡")
        
        var_type = console.input("é€‰æ‹© (1-2): ").strip()
        
        if var_type == "1":
            console.print(f"å¯ç”¨æ•°å€¼å˜é‡: {numerical_cols}")
            selected = console.input("é€‰æ‹©å˜é‡ (ç”¨é€—å·åˆ†éš”): ").strip().split(',')
            selected = [col.strip() for col in selected if col.strip() in numerical_cols]
            
            for col in selected:
                self._plot_univariate_numerical(data, col)
        
        elif var_type == "2":
            if categorical_cols:
                console.print(f"å¯ç”¨åˆ†ç±»å˜é‡: {categorical_cols}")
                selected = console.input("é€‰æ‹©å˜é‡: ").strip()
                if selected in categorical_cols:
                    self._plot_univariate_categorical(data, selected)
            else:
                console.print("[red]æ²¡æœ‰åˆ†ç±»å˜é‡[/red]")
    
    def _plot_univariate_numerical(self, data, column):
        """ç»˜åˆ¶æ•°å€¼å˜é‡å•å˜é‡å›¾"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ç›´æ–¹å›¾', 'ç®±çº¿å›¾', 'Q-Qå›¾', 'æ ¸å¯†åº¦ä¼°è®¡'),
            specs=[[{'type': 'xy'}, {'type': 'xy'}],
                   [{'type': 'xy'}, {'type': 'xy'}]]
        )
        
        # ç›´æ–¹å›¾
        fig.add_trace(
            go.Histogram(x=data[column].dropna(), name='Histogram'),
            row=1, col=1
        )
        
        # ç®±çº¿å›¾
        fig.add_trace(
            go.Box(y=data[column].dropna(), name='Box Plot'),
            row=1, col=2
        )
        
        # Q-Qå›¾
        from scipy import stats
        qq = stats.probplot(data[column].dropna(), dist="norm")
        x = qq[0][0]
        y = qq[0][1]
        
        fig.add_trace(
            go.Scatter(x=x, y=y, mode='markers', name='Q-Q Plot'),
            row=2, col=1
        )
        
        # æ·»åŠ å‚è€ƒçº¿
        fig.add_trace(
            go.Scatter(x=[x.min(), x.max()], y=[x.min(), x.max()], 
                      mode='lines', name='Normal Line', line=dict(dash='dash')),
            row=2, col=1
        )
        
        # æ ¸å¯†åº¦ä¼°è®¡
        import plotly.figure_factory as ff
        hist_data = [data[column].dropna()]
        group_labels = [column]
        
        fig_hist = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)
        
        for trace in fig_hist.data:
            fig.add_trace(trace, row=2, col=2)
        
        fig.update_layout(height=800, title_text=f"Analysis of {column}")
        fig.show()